{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "image_size = 784\n",
    "hiden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save images\n",
    "sample_dir = 'results'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(image_size, hiden_dim)\n",
    "        self.fc2_mean = nn.Linear(hiden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hiden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hiden_dim)\n",
    "        self.fc4 = nn.Linear(hiden_dim, image_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h), self.fc2_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(logvar/2) # multiply logvar by 0.5 and then take exponential\n",
    "        eps = torch.randn_like(std) # samples of the shape of standard deviation\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))  #output is in the range of 0 to 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1 gray image, 28, 28) -> (batch_size, 28*28=784)\n",
    "        mu, logvar = self.encode(x.view(-1, image_size)) #flatten the input x.view(-1, image_size)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mu, logvar\n",
    "    \n",
    "# Define model\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(states):\n",
    "    \"\"\"\n",
    "If we have a tensor z,'z.detach()' returns a tensor that shares the same storage\n",
    "as 'z', but with the computation history forgotten. It doesn't know anything\n",
    "about how it was computed. In other words, we have broken the tensor z away from its past history\n",
    "Here, we want to perform truncated Backpropagation\n",
    "TBPTT splits the 1,000-long sequence into 50 sequences (say) each of length 20 and treats each sequence of length 20 as \n",
    "a separate training case. This is a sensible approach that can work well in practice, but it is blind to temporal \n",
    "dependencies that span more than 20 timesteps.\n",
    "    \"\"\"\n",
    "    return [state.detach() for state in states] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "\n",
    "def loss_function(reconstructed_image, orifginal_image, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(reconstructed_image, orifginal_image.view(-1, image_size), reduction='sum') #reduction='sum' to get the sum of the loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) #KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "# Train model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        images = images.view(-1, image_size)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_image, mu, logvar = model(images)\n",
    "        loss = loss_function(reconstructed_image, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.3f}'.format(epoch, epochs, i, len(train_loader), loss.item()/len(images)))\n",
    "    \n",
    "    print('Epoch [{}/{}], Average Loss: {:.3f}'.format(epoch, epochs, train_loss/len(train_loader.dataset)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (images, _) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            reconstructed_image, mu, logvar = model(images)\n",
    "            test_loss += loss_function(reconstructed_image, images, mu, logvar).item()\n",
    "            \n",
    "            if batch_index == 0: #first batch at each epoch\n",
    "                n = min(images.size(0), 8)\n",
    "                comparison = torch.cat([images[:n], reconstructed_image.view(batch_size, 1, 28, 28)[:n]]) #compare original and reconstructed images -- just take the first n images\n",
    "                save_image(comparison.cpu(), os.path.join(sample_dir, 'reconstruction_image_{}.png'.format(epoch)), nrow=n)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test Loss: {:.3f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [0/469], Loss: 104.146\n",
      "Epoch [1/10], Batch [100/469], Loss: 103.160\n",
      "Epoch [1/10], Batch [200/469], Loss: 105.547\n",
      "Epoch [1/10], Batch [300/469], Loss: 106.239\n",
      "Epoch [1/10], Batch [400/469], Loss: 105.720\n",
      "Epoch [1/10], Average Loss: 105.031\n",
      "Test Loss: 104.736\n",
      "Epoch [2/10], Batch [0/469], Loss: 109.489\n",
      "Epoch [2/10], Batch [100/469], Loss: 103.378\n",
      "Epoch [2/10], Batch [200/469], Loss: 109.503\n",
      "Epoch [2/10], Batch [300/469], Loss: 106.278\n",
      "Epoch [2/10], Batch [400/469], Loss: 107.800\n",
      "Epoch [2/10], Average Loss: 104.799\n",
      "Test Loss: 104.428\n",
      "Epoch [3/10], Batch [0/469], Loss: 105.047\n",
      "Epoch [3/10], Batch [100/469], Loss: 106.196\n",
      "Epoch [3/10], Batch [200/469], Loss: 104.216\n",
      "Epoch [3/10], Batch [300/469], Loss: 109.087\n",
      "Epoch [3/10], Batch [400/469], Loss: 102.323\n",
      "Epoch [3/10], Average Loss: 104.589\n",
      "Test Loss: 104.253\n",
      "Epoch [4/10], Batch [0/469], Loss: 103.375\n",
      "Epoch [4/10], Batch [100/469], Loss: 106.833\n",
      "Epoch [4/10], Batch [200/469], Loss: 107.945\n",
      "Epoch [4/10], Batch [300/469], Loss: 104.632\n",
      "Epoch [4/10], Batch [400/469], Loss: 101.699\n",
      "Epoch [4/10], Average Loss: 104.416\n",
      "Test Loss: 104.114\n",
      "Epoch [5/10], Batch [0/469], Loss: 108.518\n",
      "Epoch [5/10], Batch [100/469], Loss: 101.646\n",
      "Epoch [5/10], Batch [200/469], Loss: 105.694\n",
      "Epoch [5/10], Batch [300/469], Loss: 103.164\n",
      "Epoch [5/10], Batch [400/469], Loss: 109.081\n",
      "Epoch [5/10], Average Loss: 104.272\n",
      "Test Loss: 104.026\n",
      "Epoch [6/10], Batch [0/469], Loss: 105.666\n",
      "Epoch [6/10], Batch [100/469], Loss: 103.449\n",
      "Epoch [6/10], Batch [200/469], Loss: 105.060\n",
      "Epoch [6/10], Batch [300/469], Loss: 103.149\n",
      "Epoch [6/10], Batch [400/469], Loss: 107.710\n",
      "Epoch [6/10], Average Loss: 104.073\n",
      "Test Loss: 103.822\n",
      "Epoch [7/10], Batch [0/469], Loss: 106.314\n",
      "Epoch [7/10], Batch [100/469], Loss: 102.253\n",
      "Epoch [7/10], Batch [200/469], Loss: 106.076\n",
      "Epoch [7/10], Batch [300/469], Loss: 103.131\n",
      "Epoch [7/10], Batch [400/469], Loss: 106.751\n",
      "Epoch [7/10], Average Loss: 103.976\n",
      "Test Loss: 103.773\n",
      "Epoch [8/10], Batch [0/469], Loss: 104.112\n",
      "Epoch [8/10], Batch [100/469], Loss: 104.540\n",
      "Epoch [8/10], Batch [200/469], Loss: 105.490\n",
      "Epoch [8/10], Batch [300/469], Loss: 104.501\n",
      "Epoch [8/10], Batch [400/469], Loss: 106.471\n",
      "Epoch [8/10], Average Loss: 103.865\n",
      "Test Loss: 103.594\n",
      "Epoch [9/10], Batch [0/469], Loss: 105.675\n",
      "Epoch [9/10], Batch [100/469], Loss: 103.207\n",
      "Epoch [9/10], Batch [200/469], Loss: 104.210\n",
      "Epoch [9/10], Batch [300/469], Loss: 101.287\n",
      "Epoch [9/10], Batch [400/469], Loss: 106.709\n",
      "Epoch [9/10], Average Loss: 103.735\n",
      "Test Loss: 103.448\n",
      "Epoch [10/10], Batch [0/469], Loss: 105.382\n",
      "Epoch [10/10], Batch [100/469], Loss: 102.831\n",
      "Epoch [10/10], Batch [200/469], Loss: 103.380\n",
      "Epoch [10/10], Batch [300/469], Loss: 106.204\n",
      "Epoch [10/10], Batch [400/469], Loss: 105.908\n",
      "Epoch [10/10], Average Loss: 103.659\n",
      "Test Loss: 103.401\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get rid off the encoder and sample some values from gaussian distribution as input to the decoder\n",
    "        # This will generate the new images\n",
    "        sample = torch.randn(64, latent_dim).to(device) #sample 64 images\n",
    "        sample = model.decode(sample).cpu() #generate images\n",
    "        save_image(sample.view(64, 1, 28, 28), os.path.join(sample_dir, 'sample_image_{}.png'.format(epoch))) #save the images and reshape them to 1 channel, 28x28 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improvementa\n",
    "#1. Add more layers to the model    \n",
    "#2. Use dropouts\n",
    "#3. Use learning grade decay\n",
    "#4. Use more epochs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
