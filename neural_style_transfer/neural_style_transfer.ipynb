{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iamge(path, image_transform, size = (300, 300)):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize(size, Image.LANCZOS)\n",
    "    image = image_transform(image).unsqueeze(0) # add batch dimension\n",
    "    return image.to(device)\n",
    "\n",
    "def get_gram(m): #m is of shape (C, H, W)\n",
    "    \"\"\"\n",
    "    m is of shape (i, C, H, W) where a is the batch size\n",
    "    \"\"\"\n",
    "    bs, c, h, w = m.size()\n",
    "    m = m.view(c, h * w)\n",
    "    m = torch.mm(m, m.t()) #metric multiplication\n",
    "    return m\n",
    "\n",
    "def denormalize_img(img):\n",
    "    img = img.transpose(1, 2, 0) # (C, H, W) -> (H, W, C)\n",
    "    mean = np.array((0.485, 0.456, 0.406))\n",
    "    std = np.array((0.229, 0.224, 0.225))\n",
    "    img = std * img + mean #denormalize image\n",
    "    img = np.clip(img, 0, 1) # values should be between 0 and 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.selected_layers = [3,8,15,22] # layers to extract features from, choose layers after relu activation. Relu adds non-linearity to the feature maps, it is import for detecting patterns in the image\n",
    "        self.vgg = models.vgg16(pretrained=True).features #pretrained VGG16 model and extract features from this model\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_features = []\n",
    "        for layer_num, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if int(layer_num) in self.selected_layers:\n",
    "                layer_features.append(x)\n",
    "        return layer_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "content_img = get_iamge('dancing.jpg', img_transform, size=(300, 300))\n",
    "style_img = get_iamge('picasso.jpg', img_transform, size=(300, 300))\n",
    "\n",
    "generated_img = content_img.clone().requires_grad_(True) # clone content image and set requires_grad to True, it should be learnable parameter\n",
    "\n",
    "optimizer = torch.optim.Adam([generated_img], lr=0.003, betas=[0.5, 0.999]) \n",
    "encoder = FeatureExtractor().to(device)\n",
    "\n",
    "for p in encoder.parameters(): # freeze the encoder parameters\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Content Loss: 0.0000, Style Loss: 160.9563\n",
      "Epoch: 10, Content Loss: 0.6700, Style Loss: 113.7369\n",
      "Epoch: 20, Content Loss: 0.8613, Style Loss: 89.9734\n",
      "Epoch: 30, Content Loss: 0.9257, Style Loss: 73.6981\n",
      "Epoch: 40, Content Loss: 0.9661, Style Loss: 60.4524\n",
      "Epoch: 50, Content Loss: 1.0006, Style Loss: 49.3137\n",
      "Epoch: 60, Content Loss: 1.0256, Style Loss: 40.2370\n",
      "Epoch: 70, Content Loss: 1.0449, Style Loss: 33.0206\n",
      "Epoch: 80, Content Loss: 1.0592, Style Loss: 27.3152\n",
      "Epoch: 90, Content Loss: 1.0716, Style Loss: 22.7654\n",
      "Epoch: 100, Content Loss: 1.0831, Style Loss: 19.0817\n",
      "Epoch: 110, Content Loss: 1.0942, Style Loss: 16.0543\n",
      "Epoch: 120, Content Loss: 1.1046, Style Loss: 13.5466\n",
      "Epoch: 130, Content Loss: 1.1147, Style Loss: 11.4585\n",
      "Epoch: 140, Content Loss: 1.1239, Style Loss: 9.7144\n",
      "Epoch: 150, Content Loss: 1.1331, Style Loss: 8.2568\n",
      "Epoch: 160, Content Loss: 1.1421, Style Loss: 7.0427\n",
      "Epoch: 170, Content Loss: 1.1518, Style Loss: 6.0320\n",
      "Epoch: 180, Content Loss: 1.1613, Style Loss: 5.1920\n",
      "Epoch: 190, Content Loss: 1.1699, Style Loss: 4.4950\n",
      "Epoch: 200, Content Loss: 1.1786, Style Loss: 3.9177\n",
      "Epoch: 210, Content Loss: 1.1878, Style Loss: 3.4400\n",
      "Epoch: 220, Content Loss: 1.1962, Style Loss: 3.0447\n",
      "Epoch: 230, Content Loss: 1.2038, Style Loss: 2.7174\n",
      "Epoch: 240, Content Loss: 1.2113, Style Loss: 2.4457\n",
      "Epoch: 250, Content Loss: 1.2186, Style Loss: 2.2190\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m      6\u001b[0m     content_features \u001b[38;5;241m=\u001b[39m encoder(content_img)\n\u001b[0;32m----> 7\u001b[0m     style_features \u001b[38;5;241m=\u001b[39m encoder(style_img)\n\u001b[1;32m      8\u001b[0m     generated_features \u001b[38;5;241m=\u001b[39m encoder(generated_img)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# content loss -> we are going to do this at the last layer only\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m layer_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_num, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgg\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(layer_num) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_layers:\n\u001b[1;32m     13\u001b[0m         layer_features\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    165\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, ceil_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mceil_mode,\n\u001b[1;32m    166\u001b[0m                         return_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_indices)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "content_weight =1\n",
    "style_weight = 100\n",
    "\n",
    "for epoch in range(500):\n",
    "    \n",
    "    content_features = encoder(content_img)\n",
    "    style_features = encoder(style_img)\n",
    "    generated_features = encoder(generated_img)\n",
    "\n",
    "    # content loss -> we are going to do this at the last layer only\n",
    "    content_loss = torch.mean((content_features[-1] - generated_features[-1])**2)\n",
    "\n",
    "    # for each of layers we are going to compute the gram matrix and add the style loss -> Computed at all of the selected layers\n",
    "    style_loss = 0\n",
    "    for gf, sf in zip(generated_features, style_features):\n",
    "        bs, c, h, w = gf.size()\n",
    "        gram_gf = get_gram(gf)\n",
    "        gram_sf = get_gram(sf)\n",
    "        style_loss += torch.mean((gram_gf - gram_sf)**2) / (c * h * w) # normalize the loss by dividing by the total number of elements in the feature map\n",
    "    \n",
    "    loss = content_weight * content_loss + style_loss * style_weight\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}, Content Loss: {:.4f}, Style Loss: {:.4f}'.format(epoch, content_loss.item(), style_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = generated_img.detach().cpu().squeeze()\n",
    "denorm_img = denormalize_img(inp)\n",
    "plt.imshow(denorm_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
