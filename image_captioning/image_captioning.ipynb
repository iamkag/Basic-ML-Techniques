{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nz/yx4776yn66n8sjwvrtfd_w3r0000gn/T/ipykernel_1805/925229239.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint( epoch, encoder, decoder, decoder_optimizer,\n",
    "                    bleu4, is_best):\n",
    "    state = {'epoch': epoch,\n",
    "             'encoder': encoder,\n",
    "             'decoder': decoder,\n",
    "             'decoder_optimizer': decoder_optimizer}\n",
    "    filename = 'checkpoint_' + str(epoch) + '.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust learning rate\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is: {:.3f}\".format(optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets, k):\n",
    "    # k is the number of top k predictions to consider\n",
    "    \"\"\"\n",
    "    predictions: batch_size , max(decode_lengths) , vocab_size ->> ((sum of decode_lengths) , vocab_size)\n",
    "    targets: batch_size , max(decode_lengths) ->> (sum of decode_lengths)\n",
    "    \"\"\"\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = predictions.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  tensor([[-0.0576,  0.6382,  0.5079,  0.6549,  1.9575,  0.0989,  1.0490],\n",
      "        [ 0.0279, -0.2752, -1.0813, -0.7613,  1.6496,  1.0647,  1.6779],\n",
      "        [ 0.3651, -1.9554,  0.3273,  2.1166,  1.8529, -0.3284,  1.2625],\n",
      "        [-0.8952,  0.5363,  1.9846, -1.0545,  1.0618, -1.4901, -0.8384]])\n",
      "targets:  tensor([-0.3005,  0.4374,  0.0802, -1.9800])\n",
      "ind:  tensor([[4, 6, 3],\n",
      "        [6, 4, 5],\n",
      "        [3, 4, 6],\n",
      "        [2, 4, 1]])\n",
      "ind.shape:  torch.Size([4, 3])\n",
      "transforms:  tensor([[-0.3005, -0.3005, -0.3005],\n",
      "        [ 0.4374,  0.4374,  0.4374],\n",
      "        [ 0.0802,  0.0802,  0.0802],\n",
      "        [-1.9800, -1.9800, -1.9800]])\n"
     ]
    }
   ],
   "source": [
    "def example_1():\n",
    "    predictions = torch.randn(4,7)\n",
    "    targets = torch.randn(4)\n",
    "    print(\"predictions: \", predictions)\n",
    "    print(\"targets: \", targets)\n",
    "    _, ind = predictions.topk(3, 1, True, True)\n",
    "    print(\"ind: \",ind)\n",
    "    print(\"ind.shape: \",ind.shape)\n",
    "    targets = targets.view(-1, 1).expand_as(ind)\n",
    "    print(\"transforms: \",targets)\n",
    "\n",
    "example_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flicker8kDataset(Dataset):\n",
    "    def __init__(self, data_transforms):\n",
    "        # Load images\n",
    "        self.h = h5py.File('TRAIN_IMAGES.hdf5', 'r') # read an hdf5 file\n",
    "        self.img = self.h['images'] # images\n",
    "        self.cpi = self.h.attrs['captions_per_image'] # captions per image\n",
    "        # Load captions\n",
    "        with open('TRAIN_CAPTIONS.json', 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "        # Load caption lengths\n",
    "        with open('TRAIN_CAPLENS.json', 'r') as j:\n",
    "            self.caplens = json.load(j)\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.captions)\n",
    "        # Image transformer\n",
    "        self.transform = data_transforms\n",
    "\n",
    "    def __getitem__(self, i): # Retrieve one image and the caption of it\n",
    "        # i is the index of the image\n",
    "        # [i // self.cpi] this is happening because we have e.g. 5 captions per image \n",
    "        # So here the maximum range of a pixel in each of the RGMB channels is 255, so therefore we're gonna divide it by 255 to get values between zero and one\n",
    "        img = torch.FloatTensor(self.img[i // self.cpi] / 255.) \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        caption = torch.LongTensor(self.captions[i]) # captions\n",
    "        caplen = torch.LongTensor([self.caplens[i]]) # caption lengths\n",
    "        return img, caption, caplen\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size # it'll keep loading data until it reaches this number right here.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'TRAIN_IMAGES.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(Flicker8kDataset(data_transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m), \n\u001b[1;32m      2\u001b[0m                                            batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      3\u001b[0m                                            pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                                            )\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mFlicker8kDataset.__init__\u001b[0;34m(self, data_transforms)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_transforms):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN_IMAGES.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# read an hdf5 file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# images\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptions_per_image\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# captions per image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'TRAIN_IMAGES.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Flicker8kDataset(data_transforms = None), \n",
    "                                           batch_size=10, shuffle=True, \n",
    "                                           pin_memory=True\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img, caption, caplen = next(iter(train_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = torchvision.models.resnet101(pretrained=True) # pretrained ImageNet ResNet-101\n",
    "        all_modules = list(resnet.children()) # get all the modules of the resnet (all layers)\n",
    "        modules = all_modules[:-2] # get all the modules except the last two (Adaptive average pooling and the fully connected layer(Linear layer))\n",
    "        self.resnet = nn.Sequential(*modules) # create a sequential model with all the modules except the last two\n",
    "        self.avgpool = nn.AvgPool2d((14, 14)) # create an adaptive average pooling layer to convert the output of the resnet to a fixed size\n",
    "        self.fine_tune() # fine-tune the model\n",
    "    \n",
    "    \"\"\"So usually when you use a pre-trained convolutional network\n",
    "    and a encoder decoder framework,\n",
    "    then you freeze the layers in the first few training steps\n",
    "    and you just train the decoder, the LSTM decoder alone.\n",
    "    And then you fine tune the CNN along with the decoder\n",
    "    for three or four more epochs.\"\"\"\n",
    "\n",
    "    def fine_tune(self, fine_tune): \n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        images: (batch_size, 3, 254, 254)\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        encoded_image = self.resnet(images)\n",
    "        global_features = self.avgpool(encoded_image) # (batch_size, 2048, 1, 1)\n",
    "        return  global_features.view(batch_size, -1) # (batch_size, 2048)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, decoder_dim, vocab_size, encoder_dim=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # embedding layer\n",
    "        # embed_dim + encoder_dim: So in this case, each of the LSTM time steps receives the word and the image feature. So therefore, the dimensionality of the LSTM is the words, the dimensionality of the word plus the image feature dimensionality.\n",
    "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size) # Classification layer\n",
    "        # Initialize the weights\n",
    "        self.init_weights() #for faster convergence\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1) #  uniform distribution between -0.1 and 0.1\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1) \n",
    "        self.fc.bias.data.fill_(0) # Initialize bias with zeros\n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        # construct metrixs of zeros\n",
    "        h = torch.zeros(batch_size, self.decoder_dim).to(device) # Hidden state\n",
    "        c = torch.zeros(batch_size, self.decoder_dim).to(device) # Memory stage\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, global_iamge, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        global_image: (batch_size, number_pixels, 2048)\n",
    "        encoded_captions: (batch_size, max_caption_length)\n",
    "        caption_lengths: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        batch_size = global_iamge.size(0)\n",
    "        encoder_dim = global_iamge.size(-1) # (2048)\n",
    "        caption_lengths, sort_indx = caption_lengths.squeeze(1).sort(dim =0, descending =  True) # sort the caption lengths in descending order\n",
    "\n",
    "        \"\"\"Okay, so now what we're going to do is\n",
    "        sort everything again, since we've sorted the caption\n",
    "        lengths according to descending order.\n",
    "        So that means the order\n",
    "        of the batch samples is different now.\n",
    "        So therefore we need to reorder the encoded captions\n",
    "        and the global image,\n",
    "        because if we don't, then basically we have different order\n",
    "        since we've sorted them here.\n",
    "        \"\"\"\n",
    "        global_image = global_image[sort_indx]\n",
    "        encoded_captions = encoded_captions[sort_indx] # (batch_size, max_caption_length)\n",
    "        # Run embeddings layer\n",
    "        embeddings = self.embedding(encoded_captions) # (batch_size, max_caption_length, embed_dim)\n",
    "        # Initialize LSTM hidden state\n",
    "        h,c = self.init_hidden_state(batch_size)\n",
    "        decode_lengths = (caption_lengths - 1).tolist() # (batch_size) # (caption_lengths - 1) because we don't want to decode the <end> token\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(device) # (batch_size, max(decode_lengths), vocab_size)\n",
    "\n",
    "        # Define LSTM\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths]) # batch size for each time step\n",
    "            lstm_input = torch.cat([embeddings[:batch_size_t, t, :], global_image[:batch_size_t]], dim=-1) \n",
    "            h, c = self.lstm(lstm_input, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(h)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, sort_indx\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
